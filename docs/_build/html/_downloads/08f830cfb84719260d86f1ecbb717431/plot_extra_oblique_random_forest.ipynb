{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Plot extra oblique forest and oblique random forest predictions on cc18 datasets\n\nA performance comparison between extra oblique forest and standard oblique random\nforest using four datasets from OpenML benchmarking suites.\n\nExtra oblique forest uses extra oblique trees as base model which differ from classic\ndecision trees in the way they are built. When looking for the best split to\nseparate the samples of a node into two groups, random splits are drawn for each of\nthe `max_features` randomly selected features and the best split among those is chosen.\nWhen `max_features` is set 1, this amounts to building a totally random\ndecision tree.\n\nThe datasets used in this example are from the OpenML benchmarking suite are:\n\n[Phishing Website](https://www.openml.org/search?type=data&sort=runs&id=4534),\n[WDBC](https://www.openml.org/search?type=data&sort=runs&id=1510),\n[Lsvt](https://www.openml.org/search?type=data&sort=runs&id=1484),\n[har]((https://www.openml.org/search?type=data&sort=runs&id=1478), and\n[cnae-9](https://www.openml.org/search?type=data&sort=runs&id==1468).\n All datasets are subsampled due to computational constraints. Note that `cnae-9` is\n an high dimensional dataset with very sparse 856 features, mostly consisting of zeros.\n\ndataset| samples | features | datatype\n-------|---------|----------|---------\nPhishing Website | 8844 | 30 | nominal\nWDBC | 455 | 30 | numeric\nLsvt | 100 | 310 | numeric\nhar | 100 | 561 | numeric\ncnae-9 | 100 | 856 | numeric\n\n## References\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n        Machine Learning, 63(1), 3-42, 2006.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import RepeatedKFold, cross_validate\n\nfrom sktree import ExtraObliqueRandomForestClassifier, ObliqueRandomForestClassifier\n\n# Parameters\nrandom_state = 12345\nphishing_website = 4534\nwdbc = 1510\nlsvt = 1484\nhar = 1478\ncnae_9 = 1468\n\ndata_ids = [phishing_website, wdbc, lsvt, har, cnae_9]\ndf = pd.DataFrame()\n\n\ndef load_cc18(data_id):\n    df = fetch_openml(data_id=data_id, as_frame=True, parser=\"pandas\")\n\n    # extract the dataset name\n    d_name = df.details[\"name\"]\n\n    # Subsampling large datasets\n    if data_id in [1468, 1478]:\n        n = 100\n    else:\n        n = int(df.frame.shape[0] * 0.8)\n\n    df = df.frame.sample(n, random_state=random_state)\n    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n\n    return X, y, d_name\n\n\ndef get_scores(X, y, d_name, n_cv=5, n_repeats=1, **kwargs):\n    clfs = [ExtraObliqueRandomForestClassifier(**kwargs), ObliqueRandomForestClassifier(**kwargs)]\n    dim = X.shape\n    tmp = []\n\n    for i, clf in enumerate(clfs):\n        t0 = datetime.now()\n        cv = RepeatedKFold(n_splits=n_cv, n_repeats=n_repeats, random_state=kwargs[\"random_state\"])\n        test_score = cross_validate(estimator=clf, X=X, y=y, cv=cv, scoring=\"accuracy\")\n        time_taken = datetime.now() - t0\n        # convert the time taken to seconds\n        time_taken = time_taken.total_seconds()\n\n        tmp.append(\n            [\n                d_name,\n                dim,\n                [\"EORF\", \"ORF\"][i],\n                test_score[\"test_score\"],\n                test_score[\"test_score\"].mean(),\n                time_taken,\n            ]\n        )\n\n    df = pd.DataFrame(tmp, columns=[\"dataset\", \"dimension\", \"model\", \"score\", \"mean\", \"time_taken\"])\n    df = df.explode(\"score\")\n    df[\"score\"] = df[\"score\"].astype(float)\n    df.reset_index(inplace=True, drop=True)\n\n    return df\n\n\nparams = {\n    \"max_features\": None,\n    \"n_estimators\": 50,\n    \"max_depth\": None,\n    \"random_state\": random_state,\n    \"n_cv\": 10,\n    \"n_repeats\": 1,\n}\n\nfor data_id in data_ids:\n    X, y, d_name = load_cc18(data_id=data_id)\n    tmp = get_scores(X=X, y=y, d_name=d_name, **params)\n    df = pd.concat([df, tmp])\n\n# Show the time taken to train each model\nprint(df.groupby([\"dataset\", \"dimension\", \"model\"])[[\"time_taken\"]].mean())\n\n# Draw a comparison plot\nd_names = df.dataset.unique()\nN = d_names.shape[0]\n\nfig, ax = plt.subplots(1, N)\nfig.set_size_inches(6 * N, 6)\n\nfor i, name in enumerate(d_names):\n    sns.stripplot(\n        data=df.query(f'dataset == \"{name}\"'),\n        x=\"model\",\n        y=\"score\",\n        ax=ax[i],\n        dodge=True,\n    )\n    sns.boxplot(\n        data=df.query(f'dataset == \"{name}\"'),\n        x=\"model\",\n        y=\"score\",\n        ax=ax[i],\n        color=\"white\",\n    )\n    ax[i].set_title(name)\n    if i != 0:\n        ax[i].set_ylabel(\"\")\n    ax[i].set_xlabel(\"\")\n# show the figure\nplt.show()\n\n\n# Discussion\n# ----------\n# Extra Oblique Tree demonstrates performance similar to that of regular Oblique Tree on average\n# with some increase in variance.\n# However, Extra Oblique Tree runs substantially faster than Oblique Tree on some datasets due to\n# the random_splits process which omits the computationally expensive search for the best split."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}